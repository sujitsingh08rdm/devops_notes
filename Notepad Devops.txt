DEVOPS => devops is the set of practices that combines software developemnt ->  aim to automating and improving the flow of dev -> test -> deployment. So developement + operations.
build faster, depeloyement safer and easy to maintain.

PLAN -> DEVELOP -> BUILD -> TEST -> RELEASE -> DEPLOY -> MONITOR

UNIX => 
  navigation :
    pwd -> present working directory
    ls -> list all the files
    cd -> change directory
    cd Folder -> to go the Folder
    cd .. -> goes directory back
    cd / -> to goto root directory
    ls -l -> to give details of the the files
    ls -a -> to see hidden file
    touch newFile.txt -> to create a file called newFile.txt
    mkdir test -> to create a new folder/directory called test
    rm filename.txt -> to delete a file 
    rm -r foldername -> to delete a folder
    find -name "fewfile.txt" -> to find the file with name "" 
    grep "hello" newFile.txt -> to find a particular text "hello" inside a file

SHELL SCRIPTING =>
  script -> textfile -> series of commands of that the linux shell (bash) will execute.
  nano firstScriptFile.sh -> nano mean to create a shellScript file.
    /bin/bash
    echo "Hello Word"

  now exit to exit press ctrl+O -> enter -> ctrl+X

  chmod +x firstScriptFile.sh -> to make the script file executeable 
  ./firstScriptFile.sh -> to execute this script we will get "Hello Word"

  nano variables.sh 
  /bin/bash
  name = "sangam"
  echo "Hello $name!"

  execute it we will get Hello sangam!

  nano conditionals.sh
  number=5
  if [ $number -gt 5 ]; then 
    echo "The number is greater then 5"
  else
    echo "The numebr is not greater then 5"

  execute it 


GIT => 
git is a version control system that tracks changes in your source code.
github is cloud based hosting service


git branch -> to show branch
git branch "branchname" -> to create a new branch
git switch "branchname" -> to switch a branch
git switch -c "branchname" -> to create a branch and switch it
git branch -d "branchname" -> to delete branch
-now again do chanign a little code and try to publish the dev branch-
git switch dev
git add.
git commit -m "change in dev"
git push -u origin dev
-now to merge the changes to main branch from dev branch we will-
git switch main
git merge dev 
git push
git pull -> to get sync the branch from cloud to local.
git checkout -b feature_one -> creates a new branch feature_one and switches to it immediately


DOCKER => 

docker run -d -p 8080:80 nginx , to create a container , if nginx is nout found locally it will download, -d to run in detach mode(background), -p port (client:docker) 
docker ps -> show all the running containers
docker ps -a -> -a list all containers
docker stop <containerId> -> stop a running container
docker rm <containerId> -> to delete a container from docker (container should be in stop state)
docker prune -> to stop all the running docker containers
docker rmi <imagename> -> to remove an image
docker rmi -f <imagename> -> to remove an image by force
docker build -t mern-backend . -> -t tags the image that we can reference later
docker run -p 5000:5000 --name customName mern-backend ->  
docker run -p 5000:5000 --name customName --env-file.env mern-backend -> read the env filecontent 

same for frotnend

DOCKER COMPOSE => 

its a tool to comfig a multi-container app in a single-yaml
 
docker-compose up --build -d -> to run docker
docker-compose down -> to bring down everything 
docker-compose logs -> to view logs


CI =>

create a .github folder then workflows then a ci.yml file then add below code in it.

name: build-and-lint

# triggers :define when this workflow should run
# trigger on push to these branches

on:
  push:
    branches: [main, dev]
  pull_request:
    branches: [main, dev]

#define a job or multiple jobs

jobs:
  build-and-lint:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Setup node
        uses: actions/setup-node@v3
        with:
          node-version: 20

      - name: Install backend depedencies
        working-directory: server
        run: npm ci

      - name: Lint backend
        working-directory: server
        run: npm run lint -- --max-warnings=0

      - name: Install frontend depedencies
        working-directory: client
        run: npm ci

      - name: Lint frontend
        working-directory: client
        run: npm run lint -- --max-warnings=35

      - name: Build backend image
        run: docker build -t backend ./server

      - name: Build frontend image
        run: docker build -t frontend ./client

Now make changes and push the branch to github. Now we should see it. in actions.

CD =>

Now lets come to deployement using EC2 (change the network tab is the needed port 5000 and 5173 using tcp connection and then also create a keypair). create a instatace now loging to instance using 
ssh -i "C:\Users\sujit\.ssh\keypair.pem" ec2-user@13.60.210.125

install docker -> nano install_docker_amazon.sh 
inside sh

--------------------------------------------------------------------------------------------------
#! /bin/bash

echo "Updating system packages..."
sudo dnf update -y

echo "Installing Docker..."
sudo dnf install -y docker

echo "Adding ec2-user to docker group..."
sudo usermod -aG docker ec2-user

echo "Enabling and starting docker services..."
sudo systemctl enable docker
sudo systemctl start docker

echo "Verifying docker installation..."
docker --version || echo "DOcker not found in current sessions (re-login may be required)."

echo ""
echo "docker installations complete on Amazon linux 2023!"
echo "IMPORTANT : Logout and log back in (or run 'newgrpdocker')"
echo "so that 'ec2-user' can run docker without sudo."
echo ""
echo "After re-login, test with:"
echo "docker run hello-world"

--------------------------------------------------------------------------------------------------

now make that script executeable 
chmod +x install_docker_amazon.sh
./install_docker_amazon.sh to execute it
then exit then connect again
docker --version to verify docker version
now we have to install docker compose before that we have to make a directory
sudo mkdir -p /usr/local/lib/docker/cli-plugins
then 
sudo curl -SL https://github.com/docker/compose/releases/download/v2.29.2/docker-compose-linux-x86_64 -o /usr/local/lib/docker/cli-plugins/docker-compose
then make the file executeable 
sudo chmod +x /usr/local/lib/docker/cli-plugins/docker-compose
docker compose version -> to verify docker compose isntalled or not 

Now install git 
sudo dnf install -y git

Now we have to clone our repo-
git clone https://github.com/sujitsingh08rdm/devops-ecommerce-practice.git app
then nano server/.env then paste everything from your server env to this file and save
same for client side
now lets createa docker ignore file to exclude the files that are not needed in docker
inside htat ignore file add below for both client and server in client coverage wont be there

node_modules
npm-debug.log
Dockerfile
.dockerignore
.git
.gitignore
.env
coverage
dist

chnage the preview in client to 
"preview": "vite preview --host 0.0.0.0 --port 5173",

add these two line in docker file 
ARG VITE_API_URL
ENV VITE_API_URL=$VITE_API_URL

now update hte docker-compose file so taht it could pick the values from that arg
this should be on docker-compose for client

-----------------------------------------------------------------
  frontend:
    build:
      context: ./client
      args: 
        VITE_API_URL: http:/3.36.2422.54:5000/api
      container_name: frontend01
      restart: always
      ports:
        - "5173:5173"
      depends_on:
        - backend

-----------------------------------------------------------------

then commit that to main branch and pull in our aws server using git pull
for testing just press git branch 
then git pull
docker compose build -> it will start the build process
docker compose up -d
docker compose ps to view all the running instances
docker compose logs to view logs 

deployed sucessfully

CD =>

For this we have to edit our repo
got to setting then , secret and variables then action and add secrets 
EC2_HOST should be public ip 
EC2_USE should be ec2-user
EC2_KEY should be the keypass
EC2_APP_DIR should be /home/ec2-user/app

secret part is done

now add deploy part to ci.yml file as 

  deploy:
    needs: build-and-lint
    if: github.event_name == 'push'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout (optional)
        uses: actions/checkout@v4

      - name: Add EC2 host to known_hosts
        run: |
          mkdir -p ~/.ssh
          ssh-keyscan -H "$EC2_HOST" >> ~/.ssh/known_hosts
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}

      - name: Write SSH keys
        run: |
          echo "$EC2_KEY" > ec2_key.pem
          chmod 600 ec2_key.pem
        env:
          EC2_KEY: ${{ secrets.EC2_KEY }}

      - name: Deploy on EC2 (git pull + docker compose rebuild & restart)
        run: |
          ssh -i ec2_key.pem ${EC2_USER}@${EC2_HOST} << EOF
            set -e
            cd "${EC2_APP_DIR}"

            git fetch --all
            git reset --hard origin/main

            docker compose down
            docker compose up -d --build
            docker image prune -f

            docker compose ps
          EOF
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USER: ${{ secrets.EC2_USER }}
          EC2_APP_DIR: ${{ secrets.EC2_APP_DIR }}


KUBERNETES => 

Kubectl -> kubectl is a command-line tool that you (or CI/CD) use to talk to the Control Plane.

Control Plane -> This is the brain of the Kubernetes cluster. It decides:
    What containers should run
    On which machine (node) they should run
    When to restart them if they fail
    How networking & scaling should behave
Four main component inside control  Plane
Component	  ->  Role
API Server	->    Accepts commands (from kubectl)
Scheduler	  -> Assigns pods to nodes
Controller Manager -> 	Keeps cluster state correct (restart, scale, etc)
etcd	->  Database that stores cluster info

Kubelet-> This runs on every worker node (EC2, VM, server, etc.) and does what the control plane tells it. Starts and stops containers. Reports node & pod status back to the control plane. Watches for new pod assignments

Pod -> Smallest building block (a simplest deloyment unit which is going to wrap a container).

Deployment -> Its a manager of pods 

Services -> services can locate pods via label selectors


install kubectl -> winget install Kubernetes.kubectl -> kubectl version --client (to check version)
install miniKube -> winget install Kubernetes.minikube -> minikube version (to see version)
minikube start --driver=docker (this will run the cluster inside docker)
 
lets do in our local machine

docker build build -t mern-backend:dev ./server
docker build build -t mern-frontend:dev ./client
docker images

now we have to laod these two images in our miniKube
minikube image load mern-backend:dev
minikube image load mern-frontend:dev
minikube image ls -> to view all images

now we have to create deployments for backend, we are going the set the env , get the pods , and forwrad the port , then create deployment for frontend  , expose the the frontend to use the 5173 port.

kubectl create deployment backend --image=mern-backend:dev //one pod runnning your backedn
kubectl set env deployment/backend CLOUDINARY_CLOUD_NAME=CLOUD NAME CLOUDINARY_API_KEY=KEY //set env

kubectl get pods
kubectl logs deploy/backend //to view logs

kubectl port-forward deployment/backend 5000:5000 // to fowrward port
backedn deployment done

kubectl create deployment frontend --image=mern-frontend:dev
kubectl set env deployment/frontend VITE_API_URL=http://localhost:5000
kubectl expose deployment frontend --name=frontend-svc --port=80 target-port=5173 --type=NodePort


DOCKER PART 2 =>

container is disposable  , we can create -> remove -> redeploy -> inside its filesystem is gone . Any data inside the container is temporary.
Thats where docker volumes come in play.
Docker Volume => this is storage mechanism that live outside of container filesystem, and can be mounted inside the container.
so it goes like container write data in volume -> container removed, data is still in volume -> new container created then re attach the same volume -> we can see the old data.

docker volume create demo-vol -> create a volume called demo-vol
docker volume ls -> to view all volumes created
docker volume inspect volume_name -> to view the volume

so we will do a example where we start a mongo container attach this volume , write a data and stop and remove the container again crete a container and attach this volume and see if data is persistant of not

docker run -d --name demo-mongo -v demo-vol:/data/db mongo:6.0 -> to create a mongo containerw ith the demo-vol as volume 
docker exec -it demo-mongo mongosh -> It simply gives you direct access to the DB inside the container for manual querying i -> interactive (keeps STDIN open so you can type) -t â†’ allocates a terminal (TTY)

use testdb -> Switch the shell context to a database named testdb, If the database does not exist yet, MongoDB will not create it immediately it gets created only when we insert data into it
db.getName()
db.demos.insertOne({name: "demo", ts: new Date()}) -> to insert a new data
db.demos.countDocuments()
db.demos.find().pretty() -> to get the all documents 
quit() -> to quit mongosh
docker rm -f demo-mongo -> to remove the container

docker run -d --name demo-mongo2 -v demo-vol:/data/db mongo:6.0 -> create a new container 
docker exec -it demo-mongo2 mongosh

use testdb
db.getName()
db.demos.countDocuments()
db.demos.find().pretty() -> we see data is persisted
quit()

Networking => 
lets say we have 5 countainer -> container cant talk to each other unless we explicitly put in same container.
ex. banking app -> api servers talks to db over internal networks , so keep db in private network only backend server can access the DB.

bridge(default)
user defined bridge

docker network ls
docker network create mern-net -> to create a network
docker network inspect mern-net
docker network rm 516906fac9d4 -> to delete a network
docker network create mern-net

Registry => central repository for storing and distributing docker images, its like github code but for docker coantainer images

For practice we will not use the cloud mongo rather then a mongo running on docker container.
change mongo_url to mongodb://mongo:27017/taskdb

docker_compose.yml ->

------------------------------------------------

services:
  mongo:
    image: mongo:6.0
    container_name: mongo
    restart: unless-stopped
    volumes:
      - mongo-data:/data/db
    networks:
      - mern-net

  backend:
    build:
      context: ./server
      dockerfile: Dockerfile
    container_name: backend01
    restart: unless-stopped
    env_file:
      - ./server/.env
    ports:
      - "5000:5000"
    depends_on:
      - mongo
    networks:
      - mern-net
  frontend:
    build:
      context: ./client
      dockerfile: Dockerfile
      args:
        VITE_API_URL: http://localhost:5000/api
    container_name: frontend01
    restart: always
    ports:
      - "5173:5173"
    depends_on:
      - backend
    networks: 
      - mern-net

  volumes: mongo-data

  networks:
    mern-net:
      driver: bridge

--------------------------------------------------------------

then docker compose up --build -d -> to build the container
docker ps 
docker volume ls
docker volume inspect "volId"
docker network inspect 

go to http://localhost:5173/auth/login to verify 

Now lets build backend image by 
docker build -t mern-backend:local ./server

now frontend image
 docker build -t mern-frontend:local ./client --build-arg VITE_API_URL=http://localhost:5000

Now tag it for backend
docker tag mern-backend:local sujitsingh08rdm/mern-backend:1.0.0
also for frontend
docker tag mern-frontend:local sujitsingh08rdm/mern-frontend:1.0.0

Now we have to push it both frontend and backedn
using
docker push sujitsingh08rdm/mern-frontend:1.0.0 same for backend
now remove the local copy
docker rmi mern-backend:local mern-frontend:local

now we will pull using 
docker pull sujitsingh08rdm/mern-backend:1.0.0
same for frontend , but check if network is present or not using docker network create mern-net
Now we have to run my mongo with named volume so taht data persists
docker run -d --name demo-mongo --network mern-net -v mongo-data:/data/db mongo:6.0
now run the backedn with current mongo host.
docker run -d --name demo-backend --network mern-net -e MONGO_URI="mongodb://demo-mongo:27017/taskdb" -p 5000:5000 sujitsingh08rdm/mern-backend:1.0.0
same for frontend
docker run -d --name demo-frontend --network mern-net -p 5173:5173 sujitsingh08rdm/mern-frontend:1.0.0
docker ps to verify

JENKINS => 
Jenkins is a open source automation server, automates -> building app(compile, test, building images), deploying apps (push images, run ansible, start containers), running CI/CD pipelines.

key concepts -> 
JOB : basic unit of work in Jenkins, simple jobs -> freestyle jon, pipeline  job -> scripted job 
BUILD -> the execution of that job, everytime a job is run its a build.
PIPELINE -> series of stages and steps definded in code (jenkins file). ex. stages . checkout github repo -> install depedency -> build docker images -> push images -> deploy. 
NODE/AGENT -> Machine where jenkins will runs the jobs, jenkins master node 
PLUGIN -> extension that add new features, github, docker plugin
WORKSPACE -> temporary folder where jenkins uses to run the job. contains source code.
EXECUTOR -> Worker slot in jenkins job. determines how many jobs can run at once.

JENKINS FLOW ->
1. developer pushes the code to github
2. jenkins going to detect the changes -> webhook(github notify jenkins) or via polling
3. jenkins pulls the code 
4. jenkins execute the job , freestyle job or pipeline job
5. build results will be stored -> build logs, artifacts or status, numbered #1, #2
6. notifications -> via email, slack, .etc


Install jenkins in docker using
docker pull jenkins/jenkins:lts

run the container -> first port where jenkins we ui will be accesible second port is for jenkins agent, after port is the location where we will persist the data. 
docker run -d --name jenkins -u 0 -p 8080:8080 -p 50000:50000 `
  -v jenkins_home:/var/jenkins_home `
  -v /var/run/docker.sock:/var/run/docker.sock `
  jenkins/jenkins:lts

Now install docker CLI inside the jenkins container
docker exec -u 0 -it jenkins bash
apt-get update && apt-get install -y docker.io
usermod -aG docker jenkins
exit

restart jenkins container
docker restart jenkins

check if docker availe
docker exec -it jenkins docker --version
check container
docker exec -it jenkins docker ps -a

install docker compose binary
docker exec -u 0 -it jenkins bash
#create plugin directories
mkdir -p /usr/libexec/docker/cli-plugins /usr/local/lib/docker/cli-plugins
#install curl if missing
apt-get update && apt-get install -y curl
#down compose v2 binary for this coantaier OS/ARCH
curl -fL "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/libexec/docker/cli-plugins/docker-compose || \
curl -fL "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/lib/docker/cli-plugins/docker-compose
#make it executeable 
chmod +x /usr/libexec/docker/cli-plugins/docker-compose || /usr/local/lib/docker/cli-plugins/docker-compose
#show plugin file and verify version
ls -l /usr/libexec/docker/cli-plugins /usr/local/lib/docker/cli-plugins
docker compose version || echo "docker compose not available"

goto localhost 8080 to see jenkins webUI
got password:
docker exec -it jenkins cat /var/jenkins_home/secrets/initialAdminPassword
